 InsightWatch

AI-powered Influencer & Competitor Monitoring Dashboard  
Built with **React (Vite)** + **Tailwind CSS** + **FastAPI** + **Ollama (LLMs)**

---

 Live Demo

ğŸ”— Frontend: [https://ai-agent-nu-nine.vercel.app/](https://ai-agent-nu-nine.vercel.app/)  
âš™ï¸ Backend: Local API with FastAPI + Ollama

---

## âš™ï¸ Tech Stack

- **Frontend**: React, Tailwind CSS, Vite, Lucide Icons, Framer Motion  
- **Backend**: FastAPI + Python  
- **LLM Runtime**: [Ollama](https://ollama.com/) (supports models like `llama3`, `mistral`, etc.)  
- **Future**: MongoDB, LangChain, OpenAI API

---

## ğŸ§  Features

- ğŸ” Track influencers & competitors  
- ğŸ“Š Sentiment monitoring  
- âš¡ï¸ Fast UI with animations  
- ğŸ§  LLM-powered insights (via Ollama)  
- ğŸ” Backend API with FastAPI

---

## ğŸ› ï¸ Getting Started

### Frontend (React + Vite)

```bash
cd frontend
npm install
npm run dev
Backend (FastAPI + Ollama)
Ensure Ollama is installed and a model is pulled (e.g., ollama run llama3)


cd backend
python -m venv venv
source venv/bin/activate   # On Windows: venv\Scripts\activate
pip install -r requirements.txt
uvicorn main:app --reload
 Ollama Setup (LLM)
Install Ollama from https://ollama.com
Run a model locally (example with LLaMA3):


ollama pull llama3
ollama run llama3
Make sure it's accessible at http://localhost:11434 for FastAPI.

ğŸ“ Project Structure
bash
Copy
Edit
InsightWatch/
â”œâ”€â”€ frontend/    # React dashboard
â”œâ”€â”€ backend/     # FastAPI + Ollama LLM API
â”œâ”€â”€ README.md
ğŸ“„ License
